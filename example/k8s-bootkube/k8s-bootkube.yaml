apiVersion: archon.kubeup.com/v1
kind: InstanceGroup
metadata:
  name: k8s-bootkube
spec:
  replicas: 1
  selector:
    matchLabels:
      app: k8s-bootkube
  template:
    metadata:
      annotations:
        aws.archon.kubeup.com/instance-profile: "k8s-master"
        aws.archon.kubeup.com/cloud-init-s3-path: "s3://YOUR_S3_BUCKET/cloudinit/"
      labels:
        app: k8s-bootkube
        archon.kubeup.com/preallocate-public-ip: "true"
        archon.kubeup.com/preallocate-private-ip: "true"
    spec:
      networkName: k8s-net
      instanceType: t2.small
      os: CoreOS
      image: ami-65336005
      files:
      - name: bootkube-start
        path: "/opt/bootkube/bootkube-start"
        permissions: "0544"
        owner: "root"
        template: |-
          #!/bin/bash
          # Wrapper for bootkube start
          set -e
          BOOTKUBE_ACI="${BOOTKUBE_ACI:-quay.io/coreos/bootkube}"
          BOOTKUBE_VERSION="${BOOTKUBE_VERSION:-v0.3.7}"
          BOOTKUBE_ASSETS="${BOOTKUBE_ASSETS:-/var/lib/bootkube}"
          exec /usr/bin/rkt run \
            --trust-keys-from-https \
            --volume assets,kind=host,source=$BOOTKUBE_ASSETS \
            --mount volume=assets,target=/assets \
            --volume manifests,kind=host,source=/etc/kubernetes/manifests \
            --mount volume=manifests,target=/etc/kubernetes/manifests \
            $RKT_OPTS \
            ${BOOTKUBE_ACI}:${BOOTKUBE_VERSION} --net=host --exec=/bootkube -- start --asset-dir=/assets --experimental-self-hosted-etcd --etcd-server=http://127.0.0.1:12379 "$@"
      - name: unit-bootkube
        path: "/coreos/unit/bootkube"
        template: |-
          name: bootkube.service
          command: start
          content: |-
            [Unit]
            Description=Bootstrap a Kubernetes control plane with a temp api-server
            [Service]
            Type=simple
            WorkingDirectory=/opt/bootkube
            ExecStartPre=/usr/bin/mkdir -p /srv/kubernetes/manifests
            ExecStart=/opt/bootkube/bootkube-start
      - name: unit-kubelet
        path: "/coreos/unit/kubelet"
        template: |-
          name: kubelet.service
          command: start
          enable: true
          content: |-
            [Service]
            Environment=KUBELET_VERSION={{ index .Configs "k8s" "k8s-version"}}
            Environment="RKT_OPTS=--uuid-file-save=/var/run/kubelet-pod.uuid \
              --volume dns,kind=host,source=/etc/resolv.conf \
              --mount volume=dns,target=/etc/resolv.conf \
              --volume var-log,kind=host,source=/var/log \
              --mount volume=var-log,target=/var/log \
              --volume lib-modules,kind=host,source=/lib/modules \
              --mount volume=lib-modules,target=/lib/modules \
              --volume var-cni,kind=host,source=/var/lib/cni \
              --mount volume=var-cni,target=/var/lib/cni"
            EnvironmentFile=/etc/environment
            ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
            ExecStartPre=/usr/bin/mkdir -p /var/log/containers
            ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/checkpoint-secrets
            ExecStartPre=/usr/bin/mkdir -p /var/lib/cni
            ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/run/kubelet-pod.uuid
            ExecStart=/usr/lib/coreos/kubelet-wrapper \
              --kubeconfig=/etc/kubernetes/kubeconfig \
              --require-kubeconfig \
              --lock-file=/var/run/lock/kubelet.lock \
              --exit-on-lock-contention \
              --allow-privileged=true \
              --hostname-override=${COREOS_PRIVATE_IPV4} \
              --config=/etc/kubernetes/manifests \
              --node-labels=master=true \
              --minimum-container-ttl-duration=3m0s \
              --cluster-dns={{ index .Configs "k8s" "dns-service-ip"}} \
              --node-ip={{.Status.PrivateIP}} \
              --cluster-domain=cluster.local \
              --network-plugin=kubenet
            ExecStop=-/usr/bin/rkt stop --uuid-file=/var/run/kubelet-pod.uuid
            Restart=always
            RestartSec=10
            User=root
            [Install]
            WantedBy=multi-user.target
      - name: etcd-operator.yaml
        path: "/var/lib/bootkube/manifests/etcd-operator.yaml"
        permissions: "0644"
        owner: "root"
        template: |-
          apiVersion: extensions/v1beta1
          kind: Deployment
          metadata:
            name: etcd-operator
            namespace: kube-system
            labels:
              k8s-app: etcd-operator
          spec:
            replicas: 1
            template:
              metadata:
                labels:
                  k8s-app: etcd-operator
              spec:
                containers:
                - name: etcd-operator
                  image: quay.io/coreos/etcd-operator:c391d8b7638deb81aa877773a0acce389f602415
                  env:
                  - name: MY_POD_NAMESPACE
                    valueFrom:
                      fieldRef:
                        fieldPath: metadata.namespace
      - name: etcd-service.yaml
        path: "/var/lib/bootkube/manifests/etcd-service.yaml"
        permissions: "0644"
        owner: "root"
        template: |-
          apiVersion: v1
          kind: Service
          metadata:
            name: etcd-service
            namespace: kube-system
          spec:
            selector:
              app: etcd
              etcd_cluster: kube-etcd
            clusterIP: 10.3.0.15
            ports:
            - name: client
              port: 2379
              protocol: TCP
      - name: kube-apiserver-secret.yaml
        path: "/var/lib/bootkube/manifests/kube-apiserver-secret.yaml"
        permissions: "0600"
        owner: "root"
        template: |-
          apiVersion: v1
          data:
            apiserver.crt: {{ index .Secrets "apiserver" "tls-cert" | printf "%s" | b64enc }}
            apiserver.key: {{ index .Secrets "apiserver" "tls-key" | printf "%s" | b64enc }}
            ca.crt: {{ index .Configs "k8s" "ca-cert" | b64enc }}
          kind: Secret
          metadata:
            name: kube-apiserver
            namespace: kube-system
          type: Opaque
      - name: kube-apiserver.yaml
        path: "/var/lib/bootkube/manifests/kube-apiserver.yaml"
        permissions: "0644"
        owner: "root"
        template: |-
          apiVersion: "extensions/v1beta1"
          kind: DaemonSet
          metadata:
            name: kube-apiserver
            namespace: kube-system
            labels:
              k8s-app: kube-apiserver
          spec:
            template:
              metadata:
                labels:
                  k8s-app: kube-apiserver
                annotations:
                  checkpointer.alpha.coreos.com/checkpoint: "true"
              spec:
                nodeSelector:
                  master: "true"
                hostNetwork: true
                containers:
                - name: kube-apiserver
                  image: {{ index .Configs "k8s" "hyper-kube-img" }}
                  command:
                  - /usr/bin/flock
                  - --exclusive
                  - --timeout=30
                  - /var/lock/api-server.lock
                  - /hyperkube
                  - apiserver
                  - --bind-address=0.0.0.0
                  - --secure-port=443
                  - --insecure-port=8080
                  - --advertise-address=$(POD_IP)
                  - --etcd-servers=http://10.3.0.15:2379
                  - --storage-backend=etcd3
                  - --enable-garbage-collector=false
                  - --allow-privileged=true
                  - --service-cluster-ip-range=10.3.0.0/24
                  - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota
                  - --runtime-config=api/all=true
                  - --tls-cert-file=/etc/kubernetes/secrets/apiserver.crt
                  - --tls-private-key-file=/etc/kubernetes/secrets/apiserver.key
                  - --service-account-key-file=/etc/kubernetes/secrets/apiserver.key
                  - --client-ca-file=/etc/kubernetes/secrets/ca.crt
                  - --cloud-provider=aws
                  - --anonymous-auth=false
                  env:
                  - name: POD_IP
                    valueFrom:
                      fieldRef:
                        fieldPath: status.podIP
                  volumeMounts:
                  - mountPath: /etc/ssl/certs
                    name: ssl-certs-host
                    readOnly: true
                  - mountPath: /etc/kubernetes/secrets
                    name: secrets
                    readOnly: true
                  - mountPath: /var/lock
                    name: var-lock
                    readOnly: false
                volumes:
                - name: ssl-certs-host
                  hostPath:
                    path: /usr/share/ca-certificates
                - name: secrets
                  secret:
                    secretName: kube-apiserver
                - name: var-lock
                  hostPath:
                    path: /var/lock
      - name: kube-controller-manager-disruption.yaml
        path: "/var/lib/bootkube/manifests/kube-controller-manager-disruption.yaml"
        permissions: "0644"
        owner: "root"
        template: |-
          apiVersion: policy/v1beta1
          kind: PodDisruptionBudget
          metadata:
            name: kube-controller-manager
            namespace: kube-system
          spec:
            minAvailable: 1
            selector:
              matchLabels:
                k8s-app: kube-controller-manager
      - name: kube-controller-manager-secret.yaml
        path: "/var/lib/bootkube/manifests/kube-controller-manager-secret.yaml"
        permissions: "0600"
        owner: "root"
        template: |-
          apiVersion: v1
          data:
            ca.crt: {{ index .Configs "k8s" "ca-cert" | b64enc }}
            apiserver.key: {{ index .Secrets "apiserver" "tls-key" | printf "%s" | b64enc }}
          kind: Secret
          metadata:
            name: kube-controller-manager
            namespace: kube-system
          type: Opaque
      - name: kube-controller-manager.yaml
        path: "/var/lib/bootkube/manifests/kube-controller-manager.yaml"
        permissions: "0644"
        owner: "root"
        template: |-
          apiVersion: extensions/v1beta1
          kind: Deployment
          metadata:
            name: kube-controller-manager
            namespace: kube-system
            labels:
              k8s-app: kube-controller-manager
          spec:
            replicas: 2
            template:
              metadata:
                labels:
                  k8s-app: kube-controller-manager
              spec:
                nodeSelector:
                  master: "true"
                containers:
                - name: kube-controller-manager
                  image: {{ index .Configs "k8s" "hyper-kube-img" }}
                  command:
                  - ./hyperkube
                  - controller-manager
                  - --allocate-node-cidrs=true
                  - --configure-cloud-routes=false
                  - --cluster-cidr=10.2.0.0/16
                  - --root-ca-file=/etc/kubernetes/secrets/ca.crt
                  - --service-account-private-key-file=/etc/kubernetes/secrets/apiserver.key
                  - --leader-elect=true
                  - --cloud-provider=aws
                  - --enable-garbage-collector=false
                  - --configure-cloud-routes=false
                  volumeMounts:
                  - name: secrets
                    mountPath: /etc/kubernetes/secrets
                    readOnly: true
                  - name: ssl-host
                    mountPath: /etc/ssl/certs
                    readOnly: true
                volumes:
                - name: secrets
                  secret:
                    secretName: kube-controller-manager
                - name: ssl-host
                  hostPath:
                    path: /usr/share/ca-certificates
                dnsPolicy: Default # Don't use cluster DNS.
      - name: kube-dns-deployment.yaml
        path: "/var/lib/bootkube/manifests/kube-dns-deployment.yaml"
        permissions: "0644"
        owner: "root"
        template: |-
          apiVersion: extensions/v1beta1
          kind: Deployment
          metadata:
            name: kube-dns
            namespace: kube-system
            labels:
              k8s-app: kube-dns
              kubernetes.io/cluster-service: "true"
          spec:
            # replicas: not specified here:
            # 1. In order to make Addon Manager do not reconcile this replicas parameter.
            # 2. Default is 1.
            # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on.
            strategy:
              rollingUpdate:
                maxSurge: 10%
                maxUnavailable: 0
            selector:
              matchLabels:
                k8s-app: kube-dns
            template:
              metadata:
                labels:
                  k8s-app: kube-dns
                annotations:
                  scheduler.alpha.kubernetes.io/critical-pod: ''
                  scheduler.alpha.kubernetes.io/tolerations: '[{"key":"CriticalAddonsOnly", "operator":"Exists"}]'
              spec:
                containers:
                - name: kubedns
                  image: gcr.io/google_containers/kubedns-amd64:1.9
                  resources:
                    # TODO: Set memory limits when we've profiled the container for large
                    # clusters, then set request = limit to keep this container in
                    # guaranteed class. Currently, this container falls into the
                    # "burstable" category so the kubelet doesn't backoff from restarting it.
                    limits:
                      memory: 170Mi
                    requests:
                      cpu: 100m
                      memory: 70Mi
                  livenessProbe:
                    httpGet:
                      path: /healthz-kubedns
                      port: 8080
                      scheme: HTTP
                    initialDelaySeconds: 60
                    timeoutSeconds: 5
                    successThreshold: 1
                    failureThreshold: 5
                  readinessProbe:
                    httpGet:
                      path: /readiness
                      port: 8081
                      scheme: HTTP
                    # we poll on pod startup for the Kubernetes master service and
                    # only setup the /readiness HTTP server once that's available.
                    initialDelaySeconds: 3
                    timeoutSeconds: 5
                  args:
                  - --domain=cluster.local.
                  - --dns-port=10053
                  - --config-map=kube-dns
                  # This should be set to v=2 only after the new image (cut from 1.5) has
                  # been released, otherwise we will flood the logs.
                  - --v=0
                  env:
                  - name: PROMETHEUS_PORT
                    value: "10055"
                  ports:
                  - containerPort: 10053
                    name: dns-local
                    protocol: UDP
                  - containerPort: 10053
                    name: dns-tcp-local
                    protocol: TCP
                  - containerPort: 10055
                    name: metrics
                    protocol: TCP
                - name: dnsmasq
                  image: gcr.io/google_containers/kube-dnsmasq-amd64:1.4
                  livenessProbe:
                    httpGet:
                      path: /healthz-dnsmasq
                      port: 8080
                      scheme: HTTP
                    initialDelaySeconds: 60
                    timeoutSeconds: 5
                    successThreshold: 1
                    failureThreshold: 5
                  args:
                  - --cache-size=1000
                  - --no-resolv
                  - --server=/cluster.local/127.0.0.1#10053
                  - --server=169.254.169.253
                  - --log-facility=-
                  ports:
                  - containerPort: 53
                    name: dns
                    protocol: UDP
                  - containerPort: 53
                    name: dns-tcp
                    protocol: TCP
                  # see: https://github.com/kubernetes/kubernetes/issues/29055 for details
                  resources:
                    requests:
                      cpu: 150m
                      memory: 10Mi
                - name: dnsmasq-metrics
                  image: gcr.io/google_containers/dnsmasq-metrics-amd64:1.0
                  livenessProbe:
                    httpGet:
                      path: /metrics
                      port: 10054
                      scheme: HTTP
                    initialDelaySeconds: 60
                    timeoutSeconds: 5
                    successThreshold: 1
                    failureThreshold: 5
                  args:
                  - --v=2
                  - --logtostderr
                  ports:
                  - containerPort: 10054
                    name: metrics
                    protocol: TCP
                  resources:
                    requests:
                      memory: 10Mi
                - name: healthz
                  image: gcr.io/google_containers/exechealthz-amd64:1.2
                  resources:
                    limits:
                      memory: 50Mi
                    requests:
                      cpu: 10m
                      # Note that this container shouldn't really need 50Mi of memory. The
                      # limits are set higher than expected pending investigation on #29688.
                      # The extra memory was stolen from the kubedns container to keep the
                      # net memory requested by the pod constant.
                      memory: 50Mi
                  args:
                  - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1 >/dev/null
                  - --url=/healthz-dnsmasq
                  - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 >/dev/null
                  - --url=/healthz-kubedns
                  - --port=8080
                  - --quiet
                  ports:
                  - containerPort: 8080
                    protocol: TCP
                dnsPolicy: Default  # Don't use cluster DNS.
      - name: kube-dns-svc.yaml
        path: "/var/lib/bootkube/manifests/kube-dns-svc.yaml"
        permissions: "0644"
        owner: "root"
        template: |-
          apiVersion: v1
          kind: Service
          metadata:
            name: kube-dns
            namespace: kube-system
            labels:
              k8s-app: kube-dns
              kubernetes.io/cluster-service: "true"
              kubernetes.io/name: "KubeDNS"
          spec:
            selector:
              k8s-app: kube-dns
            clusterIP: 10.3.0.10
            ports:
            - name: dns
              port: 53
              protocol: UDP
            - name: dns-tcp
              port: 53
              protocol: TCP
      - name: kube-proxy.yaml
        path: "/var/lib/bootkube/manifests/kube-proxy.yaml"
        permissions: "0644"
        owner: "root"
        template: |-
          apiVersion: "extensions/v1beta1"
          kind: DaemonSet
          metadata:
            name: kube-proxy
            namespace: kube-system
            labels:
              k8s-app: kube-proxy
          spec:
            template:
              metadata:
                labels:
                  k8s-app: kube-proxy
              spec:
                hostNetwork: true
                containers:
                - name: kube-proxy
                  image: {{ index .Configs "k8s" "hyper-kube-img" }}
                  command:
                  - /hyperkube
                  - proxy
                  - --kubeconfig=/etc/kubernetes/kubeconfig
                  - --proxy-mode=iptables
                  - --hostname-override=$(NODE_NAME)
                  - --cluster-cidr=10.2.0.0/16
                  env:
                    - name: NODE_NAME
                      valueFrom:
                        fieldRef:
                          fieldPath: spec.nodeName
                  securityContext:
                    privileged: true
                  volumeMounts:
                  - mountPath: /etc/ssl/certs
                    name: ssl-certs-host
                    readOnly: true
                  - name: etc-kubernetes
                    mountPath: /etc/kubernetes
                    readOnly: true
                volumes:
                - hostPath:
                    path: /usr/share/ca-certificates
                  name: ssl-certs-host
                - name: etc-kubernetes
                  hostPath:
                    path: /etc/kubernetes
      - name: kube-scheduler-disruption.yaml
        path: "/var/lib/bootkube/manifests/kube-scheduler-disruption.yaml"
        permissions: "0644"
        owner: "root"
        template: |-
          apiVersion: policy/v1beta1
          kind: PodDisruptionBudget
          metadata:
            name: kube-scheduler
            namespace: kube-system
          spec:
            minAvailable: 1
            selector:
              matchLabels:
                k8s-app: kube-scheduler
      - name: kube-scheduler.yaml
        path: "/var/lib/bootkube/manifests/kube-scheduler.yaml"
        permissions: "0644"
        owner: "root"
        template: |-
          apiVersion: extensions/v1beta1
          kind: Deployment
          metadata:
            name: kube-scheduler
            namespace: kube-system
            labels:
              k8s-app: kube-scheduler
          spec:
            replicas: 2
            template:
              metadata:
                labels:
                  k8s-app: kube-scheduler
              spec:
                containers:
                - name: kube-scheduler
                  image: {{ index .Configs "k8s" "hyper-kube-img" }}
                  command:
                  - ./hyperkube
                  - scheduler
                  - --leader-elect=true
      - name: pod-checkpoint-installer.yaml
        path: "/var/lib/bootkube/manifests/pod-checkpoint-installer.yaml"
        permissions: "0644"
        owner: "root"
        template: |-
          apiVersion: "extensions/v1beta1"
          kind: DaemonSet
          metadata:
            name: checkpoint-installer
            namespace: kube-system
            labels:
              k8s-app: pod-checkpoint-installer
          spec:
            template:
              metadata:
                labels:
                  k8s-app: pod-checkpoint-installer
              spec:
                nodeSelector:
                  master: "true"
                hostNetwork: true
                containers:
                - name: checkpoint-installer
                  image: quay.io/coreos/pod-checkpointer:5b585a2d731173713fa6871c436f6c53fa17f754
                  command:
                  - /checkpoint-installer.sh
                  volumeMounts:
                  - mountPath: /etc/kubernetes/manifests
                    name: etc-k8s-manifests
                volumes:
                - name: etc-k8s-manifests
                  hostPath:
                    path: /etc/kubernetes/manifests
      - name: kubeconfig
        path: "/etc/kubernetes/kubeconfig"
        permissions: "0600"
        owner: "root"
        template: |-
          apiVersion: v1
          kind: Config
          clusters:
          - name: local
            cluster:
              server: https://127.0.0.1:443
              certificate-authority-data: {{ index .Configs "k8s" "ca-cert" | b64enc }}
          users:
          - name: kubelet
            user:
              client-certificate-data: {{ index .Secrets "apiserver" "tls-cert" | printf "%s" | b64enc }}
              client-key-data: {{ index .Secrets "apiserver" "tls-key" | printf "%s" | b64enc }}
          contexts:
          - context:
              cluster: local
              user: kubelet
      - name: apiserver.crt
        path: "/var/lib/bootkube/tls/apiserver.crt"
        permissions: "0600"
        owner: "root"
        template: |-
          {{ index .Secrets "apiserver" "tls-cert" | printf "%s" }}
      - name: apiserver.key
        path: "/var/lib/bootkube/tls/apiserver.key"
        permissions: "0600"
        owner: "root"
        template: |-
          {{ index .Secrets "apiserver" "tls-key" | printf "%s" }}
      - name: ca.crt
        path: "/var/lib/bootkube/tls/ca.crt"
        permissions: "0600"
        owner: "root"
        template: |-
          {{ index .Configs "k8s" "ca-cert" }}
      - name: kubelet.crt
        path: "/var/lib/bootkube/tls/kubelet.crt"
        permissions: "0600"
        owner: "root"
        template: |-
          {{ index .Secrets "apiserver" "tls-cert" | printf "%s" }}
      - name: kubelet.key
        path: "/var/lib/bootkube/tls/kubelet.key"
        permissions: "0600"
        owner: "root"
        template: |-
          {{ index .Secrets "apiserver" "tls-key" | printf "%s" }}
      - name: service-account.key
        path: "/var/lib/bootkube/tls/service-account.key"
        permissions: "0600"
        owner: "root"
        template: |-
          {{ index .Secrets "apiserver" "tls-key" | printf "%s" }}
      - name: service-account.pub
        path: "/var/lib/bootkube/tls/service-account.pub"
        permissions: "0600"
        owner: "root"
        template: |-
          {{ index .Secrets "apiserver" "tls-key" | printf "%s" }}
      configs:
      - name: k8s
        data:
          hyper-kube-img: quay.io/coreos/hyperkube:v1.5.2_coreos.2
          service-ip-range: 10.3.0.0/24
          pod-ip-range: 10.2.0.0/16
          dns-service-ip: 10.3.0.10
          k8s-service-ip: 10.3.0.1
          k8s-version: v1.5.2_coreos.2
          ca-cert: |-
            PUT YOUR CA CERTIFICATE HERE
      users:
      - name: core
    secrets:
    - metadata:
        name: apiserver
        annotations:
            archon.kubeup.com/csr: |-
              {
                "CN": "{{ .Meta.Name }}",
                "hosts": [
                  "kubernetes",
                  "kubernetes.default",
                  "kubernetes.default.svc",
                  "kubernetes.default.svc.cluster.local",
                  "127.0.0.1",
                  "{{ index .Configs "k8s" "k8s-service-ip" }}",
                  "{{ .Status.PrivateIP }}",
                  "{{ .Status.PublicIP }}"
                ],
                "key": {
                  "algo": "ecdsa",
                  "size": 256
                },
                "names": [
                  {
                    "C": "US",
                    "L": "CA",
                    "ST": "San Francisco"
                  }
                ]
              }
            archon.kubeup.com/status: Pending
            archon.kubeup.com/type: csr
      type: Opaque
